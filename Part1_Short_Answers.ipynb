# Part1_Short_Answers.ipynb
# This notebook answers the short answer questions for the AI workflow assignment.

# =========================
# Import Libraries
# =========================
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score

# =========================
# 1. Problem Definition
# =========================
# Hypothetical AI Problem: Predicting student dropout rates
# Objectives:
# 1. Identify students at high risk of dropping out
# 2. Provide actionable interventions
# 3. Optimize academic support resource allocation
# Stakeholders:
# 1. University administration
# 2. Academic advisors/faculty
# KPI: Accuracy of early dropout predictions

# =========================
# 2. Data Collection & Preprocessing
# =========================
# Hypothetical Data Sources
# 1. Student academic records (grades, attendance)
# 2. Student engagement survey data

# Sample data creation
df = pd.DataFrame({
    'GPA': [3.2, 2.5, 3.8, np.nan, 2.0],
    'Attendance': [90, 75, 95, 85, np.nan],
    'Program': ['Science', 'Arts', 'Engineering', 'Science', 'Arts'],
    'dropout': [0, 1, 0, 0, 1]
})

# Preprocessing steps
# 1. Handle missing values
imputer = SimpleImputer(strategy='median')
df[['GPA', 'Attendance']] = imputer.fit_transform(df[['GPA', 'Attendance']])

# 2. Normalize numeric features
scaler = StandardScaler()
df[['GPA', 'Attendance']] = scaler.fit_transform(df[['GPA', 'Attendance']])

# 3. Encode categorical variables
df = pd.get_dummies(df, columns=['Program'])

print("Preprocessed Data:")
print(df)

# =========================
# 3. Model Development
# =========================
X = df.drop('dropout', axis=1)
y = df['dropout']

# Split data
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Random Forest model
model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# Evaluate on validation set
y_pred = model.predict(X_val)
y_proba = model.predict_proba(X_val)[:,1]

print("Validation Metrics:")
print(classification_report(y_val, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_val, y_pred))

# =========================
# 4. Evaluation & Deployment Notes
# =========================
# Evaluation metrics used:
# - F1 Score: balances precision and recall
# - ROC-AUC: discrimination ability of the model

# Concept drift: change in student behavior over time that reduces model performance
# Monitoring: track ongoing accuracy, retrain periodically

# Deployment Challenge: Scalability (predict for thousands of students in real-time)
